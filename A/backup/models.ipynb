{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e77b4e-5412-4750-ab3d-50648facff22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lemai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\lemai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\lemai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import nbimporter\n",
    "\n",
    "from preprocessing import preprocess\n",
    "from utlis import pred_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6df913f-853d-43c2-8497-399b0eccbf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInference(nn.Module):\n",
    "    def __init__(self, model_path, checkpoint=\"xlm-roberta-base\", device=\"cpu\"):\n",
    "        super(ModelInference, self).__init__()\n",
    "        self.preprocess = preprocess()\n",
    "        self.model = CustomXLMModel()\n",
    "        self.device = device\n",
    "        self.model.load_state_dict(torch.load(model_path,map_location=torch.device(device)))\n",
    "        self.model.to(device)\n",
    "    \n",
    "    def predict(self, sample):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Clean input, segment and tokenize\n",
    "            sample = self.preprocess.tokenize(sample)\n",
    "            inputs = {\"input_ids\": sample[\"input_ids\"].to(self.device),\n",
    "                        \"attention_mask\": sample[\"attention_mask\"].to(self.device)}\n",
    "\n",
    "            # Predict\n",
    "            outputs_classifier, outputs_regressor = self.model(**inputs)\n",
    "\n",
    "            # Convert to numpy array\n",
    "            outputs_classifier = outputs_classifier.cpu().numpy()\n",
    "            outputs_regressor = outputs_regressor.cpu().numpy()\n",
    "\n",
    "            # Get argmax each aspects\n",
    "            outputs_regressor = outputs_regressor.argmax(axis=-1) + 1\n",
    "\n",
    "            # Convert output to label\n",
    "            outputs = pred_to_label(outputs_classifier, outputs_regressor)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f8a6d47-f4bb-4dcd-a40d-692dc1bd4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomXLMModel(nn.Module):\n",
    "    def __init__(self, num_classification_labels=6, num_regression_neurons=30):\n",
    "        super(CustomXLMModel, self).__init__()\n",
    "        # Load a pre-trained XLM model\n",
    "        self.model = AutoModel.from_pretrained(\"xlm-roberta-base\", output_attentions=True,output_hidden_states=True)\n",
    "        \n",
    "        # Define layers\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.model.config.hidden_size * 4, num_classification_labels)\n",
    "        self.regressor = nn.Linear(self.model.config.hidden_size * 4, num_regression_neurons)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward pass through XLM model\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        outputs = torch.cat((outputs.hidden_states[-1][:, 0, ...], outputs.hidden_states[-2][:, 0, ...], outputs.hidden_states[-3][:, 0, ...], outputs.hidden_states[-4][:, 0, ...]), -1)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs_classifier = self.classifier(outputs)\n",
    "        outputs_regressor = self.regressor(outputs)\n",
    "        outputs_classifier = torch.sigmoid(outputs_classifier)\n",
    "        outputs_regressor = outputs_regressor.view(-1, 6, 5)\n",
    "        return outputs_classifier, outputs_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ad1c0d-6c1f-4a7b-990b-9c3801505552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomXLMModel_v2(nn.Module):\n",
    "    def __init__(self, num_classification_labels=6, num_regression_neurons=30):\n",
    "        super(CustomXLMModel, self).__init__()\n",
    "        # Load a pre-trained XLM model\n",
    "        self.model = AutoModel.from_pretrained(\"xlm-roberta-base\", output_attentions=True,output_hidden_states=True)\n",
    "        \n",
    "        # Define layers\n",
    "        # Layer 1\n",
    "        self.layer1 = nn.Linear(self.model.config.hidden_size * 4, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "        # Layer Classification\n",
    "        self.classifier = nn.Linear(self.model.config.hidden_size * 4, num_classification_labels)\n",
    "        self.bn2 = nn.BatchNorm1d(num_classification_labels)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "        # Layer Regression\n",
    "        self.regressor = nn.Linear(self.model.config.hidden_size * 4, num_regression_neurons)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward pass through XLM model\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        outputs = torch.cat((outputs.hidden_states[-1][:, 0, ...], outputs.hidden_states[-2][:, 0, ...], outputs.hidden_states[-3][:, 0, ...], outputs.hidden_states[-4][:, 0, ...]), -1)\n",
    "\n",
    "        # Apply layer 1\n",
    "        outputs = self.layer1(outputs)\n",
    "        outputs = F.relu(outputs)\n",
    "        outputs = self.bn1(outputs)\n",
    "        outputs = self.dropout1(outputs)\n",
    "\n",
    "        # Apply classification layer\n",
    "        outputs_classifier = self.classifier(outputs)\n",
    "        outputs_classifier = self.bn2(outputs_classifier)\n",
    "        outputs_classifier = self.dropout2(outputs_classifier)\n",
    "\n",
    "        # Apply regression layer\n",
    "        outputs_regressor = self.regressor(outputs)\n",
    "        \n",
    "        outputs_classifier = torch.sigmoid(outputs_classifier)\n",
    "        outputs_regressor = outputs_regressor.view(-1, 6, 5)\n",
    "        \n",
    "        return outputs_classifier, outputs_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f4131-faa5-4300-a027-1b41b5ce39f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, num_classification_labels=6, num_regression_neurons=30):\n",
    "        super(CustomXLMModel, self).__init__()\n",
    "        # Load a pre-trained XLM model\n",
    "        self.model = AutoModel.from_pretrained(\"vinai/phobert-base\", output_attentions=True,output_hidden_states=True)\n",
    "        \n",
    "        # Define layers\n",
    "        # Layer 1\n",
    "        self.layer1 = nn.Linear(self.model.config.hidden_size * 4, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "        # Layer Classification\n",
    "        self.classifier = nn.Linear(self.model.config.hidden_size * 4, num_classification_labels)\n",
    "        self.bn2 = nn.BatchNorm1d(num_classification_labels)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "        # Layer Regression\n",
    "        self.regressor = nn.Linear(self.model.config.hidden_size * 4, num_regression_neurons)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward pass through XLM model\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attetputs = torch.cat((outputs.hidden_states[-1][:, 0, ...], outputs.hidden_states[-2][:, 0, ...], outputs.hidden_states[-3][:, 0, ...], outputs.hidden_states[-4][:, 0, ...]), -1)\n",
    "\n",
    "        outputs = self.layer1(outputs)\n",
    "        outputs = F.relu(outputs)\n",
    "        outputs = self.bn1(outputs)\n",
    "        outputs = self.dropout1(outputs)\n",
    "\n",
    "        # Apply classification layer\n",
    "        outputs_classifier = self.classifier(outputs)\n",
    "        outputs_classifier = self.bn2(outputs_classifier)\n",
    "        outputs_classifier = self.dropout2(outputs_classifier)\n",
    "\n",
    "        # Apply regression layer\n",
    "        outputs_regressor = self.regressor(outputs)\n",
    "        \n",
    "        outputs_classifier = torch.sigmoid(outputs_classifier)\n",
    "        outputs_regressor = outputs_regressor.view(-1, 6, 5)\n",
    "        \n",
    "        return outputs_classifier, outputs_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b945df-a7bc-40ad-b369-3457fb12aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.models = nn.ModuleList(models)  # Danh sách các mô hình con\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Tạo danh sách các dự đoán từ các mô hình con\n",
    "        all_classifier_outputs = []\n",
    "        all_regressor_outputs = []\n",
    "\n",
    "        for model in self.models:\n",
    "            classifier_output, regressor_output = model(input_ids, attention_mask)\n",
    "            all_classifier_outputs.append(classifier_output)\n",
    "            all_regressor_outputs.append(regressor_output)\n",
    "\n",
    "        # Trung bình dự đoán từ tất cả các mô hình con\n",
    "        ensemble_classifier_output = torch.mean(torch.stack(all_classifier_outputs), dim=0)\n",
    "        ensemble_regressor_output = torch.mean(torch.stack(all_regressor_outputs), dim=0)\n",
    "\n",
    "        return ensemble_classifier_output, ensemble_regressor_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
