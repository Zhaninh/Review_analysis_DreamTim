{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2193909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lemai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\lemai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\lemai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from vncorenlp import VnCoreNLP\n",
    "from datasets import DatasetDict, Dataset\n",
    "import nbimporter\n",
    "from utlis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4581693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_special_keys(review):\n",
    "    special_character = re.compile(\"�+\")\n",
    "    return special_character.sub(r'', review)\n",
    "\n",
    "def rm_punctuation(review):\n",
    "    punctuation = re.compile(r\"[!#$%&()*+;<=>?@[\\]^_`{|}~]+\")\n",
    "    return punctuation.sub(r\"\", review)\n",
    "\n",
    "def rm_emoji(review):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U0001F004-\\U0001F0CF\"  # Mahjong Tiles\n",
    "        u\"\\U0001F170-\\U0001F251\"  # Enclosed Characters\n",
    "        u\"\\U0001F300-\\U0001F9F9\"  # Additional symbols and emojis\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', review)\n",
    "    return text\n",
    "\n",
    "def rm_urls_paths(text):\n",
    "    # Define a regex pattern to match both URLs and file paths\n",
    "    url_pattern = r'https?[:]//\\S+|www\\.\\S+'\n",
    "    path_pattern = r'(?:(?:[a-z]:\\\\|\\\\\\\\|/)[^\\s|/]+(?:/[^\\s|/]+)*)'\n",
    "    combined_pattern = f'({url_pattern})|({path_pattern})'\n",
    "    cleaned_text = re.sub(combined_pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def normalize_annotatation(text):\n",
    "    khach_san = \"\\bkhach san ?|\\bksan ?|\\bks ?\"\n",
    "    return re.sub(\"\\bnv ?\", \"nhân viên\",re.sub(khach_san, \"khách sạn\", text))\n",
    "\n",
    "def rm_escape_characters(text):\n",
    "    cleaned_text = text.replace('\\r', '').replace('\\n', '').replace('\\t', '').replace('\\q', '').replace('\\w', '').replace('\\s', '')\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_text(review):\n",
    "    cleaned_review = {\"Review\": rm_escape_characters(normalize_annotatation(rm_special_keys(rm_punctuation(rm_emoji(rm_urls_paths(review['Review'].lower()))))))}\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d6052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess():\n",
    "    def __init__(self):\n",
    "        self.proj_path = get_proj_path()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "        self.segmenter = VnCoreNLP(os.path.join(self.proj_path, 'vncorenlp', 'VnCoreNLP-1.1.1.jar'), annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
    "        self.feature = ['giai_tri', 'luu_tru', 'nha_hang', 'an_uong', 'di_chuyen', 'mua_sam']\n",
    "        \n",
    "    def segment(self, df):\n",
    "        return {\"Segment\": \" \".join([\" \".join(sen) for sen in self.segmenter.tokenize(df[\"Review\"])])}\n",
    "        \n",
    "    def tokenize(self, df):\n",
    "        return self.tokenizer(df[\"Segment\"], truncation=True, padding=True, max_length=165)\n",
    "    \n",
    "    def label(self, example):\n",
    "        return {'labels_regressor': np.array([example[i] for i in self.feature]),\n",
    "            'labels_classifier': np.array([int(example[i] != 0) for i in self.feature])}\n",
    "    \n",
    "    def rm_stopwords(self, text, remove_stopwords=True):\n",
    "        stopword_path = os.path.join(self.proj_path, \"vn_stopwords\", \"vietnamese-stopwords-dash.txt\")\n",
    "        with open(stopword_path, 'r', encoding='utf-8') as file:\n",
    "            stop_words = set(file.read().splitlines())    \n",
    "        words = text['Review'].split()\n",
    "        if remove_stopwords:\n",
    "            words = [word for word in words if word.lower() not in stop_words]\n",
    "        cleaned_text = ' '.join(words)\n",
    "        return {\"Review\": cleaned_text}\n",
    "        \n",
    "    def run(self, dataset):\n",
    "        dataset = dataset.map(clean_text)\n",
    "        dataset = dataset.map(self.segment)\n",
    "        dataset = dataset.map(self.tokenize, batched=True)\n",
    "        dataset = dataset.map(self.label)\n",
    "        dataset = dataset.map(self.rm_stopwords)\n",
    "        dataset.set_format(\"torch\")\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16b62bb1-f182-4eb0-936e-a8059898121a",
   "metadata": {},
   "source": [
    "data_path = r\"D:\\FSoft\\Review_Ana\\Dream_Tim\\A\\datasets\\data_original\\Original-datasets.csv\"\n",
    "train_df = pd.read_csv(data_path)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eab35cb0-7495-4679-8273-08dc90d629cf",
   "metadata": {},
   "source": [
    "new_value = 'Tôi bắt xe 7� chỗ đi từ sân bay về nhà.Thái độ của tài� xế không� vui vẻ khi đón chúng tôi.mặt thì nhăn nhó thái độ thì lơ lơ.gia đình đi \\r\\n7 người.tài xe mở cốp xe rồi để tôi tự xếp hành lý vào.sau đó dẹp lun 2 ghế sau để chất vali lên.5 người!!!@@ trong gia đình phải dồn vô ngồi ghế giữa. 2 người ngồi ghế trước.lên xe thì nóng.tôi yêu cầu tài xế%^& 😂 mở máy lạnh thì tài xế bảo cả sáng h()#% đậu ngoài nắng nên nóng.chạy\\r\\n 10p vẫn chưa thấy mở máy lạnh.mà#&^#&😂😂 trong xe nóng như cái lò 5 người ngồi chen nhau.hỏi tiếp thì không trả lời.sau đó mình yêu cầu nhiều quá mới kêu đang mở.về gần đến nhà mới thấy quạt nó thổi mát được xíu.ngồi trên xe 30p mà như cực hình.yêu cầu công ty xem xét lại thái độ làm việc của tài xế chạy xe 6898 lúc 10h sáng ngày 10 tháng 7.nghiêm túc phê bình.https://example.com or visit C:\\\\Documents\\\\file.txt. hoặc là www.example.com.vn'\n",
    "\n",
    "# Đặt giá trị mới cho hàng và cột cụ thể trong DataFrame\n",
    "train_df.at[7, 'Review']=new_value\n",
    "train_df.at[7, 'Review']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c74ec9b-4cb7-49c6-aeb3-112394bc73fa",
   "metadata": {},
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "\n",
    "# Tạo một DatasetDict mới\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset\n",
    "})\n",
    "dataset_dict['train']['Review'][7]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "847a3299-5b4e-4b2c-98ea-088d45c7bc92",
   "metadata": {},
   "source": [
    "reviews_df = dataset_dict.copy()\n",
    "reviews_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bb17403-614e-493d-af49-d52481b6bb37",
   "metadata": {},
   "source": [
    "prep = preprocess()\n",
    "tokenized_datasets = prep.run(dataset_dict)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe7ee4d1-79c7-4677-8ce7-4f472a499437",
   "metadata": {},
   "source": [
    "reviews_df['train']['Review'][7]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "965cdcd8-f690-4d44-acdf-bd6243571abb",
   "metadata": {},
   "source": [
    "tokenized_datasets['train']['Review'][7]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e30ecaf-59ac-4f4b-a76c-ddbfb385efc9",
   "metadata": {},
   "source": [
    "df = pd.DataFrame(tokenized_datasets['train'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f46873ee-f086-477e-8013-8233f72bcdd0",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a05da88-ab50-479d-a87a-71f63278685d",
   "metadata": {},
   "source": [
    "df.iloc[0].input_ids"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40636a79-9667-4af7-8f2f-8edd22bf0fb3",
   "metadata": {},
   "source": [
    "review=1\n",
    "for i in range(len(df.iloc[review].input_ids)):\n",
    "    print(f'{df.iloc[review].input_ids[i]} ---> {prep.tokenizer.decode(df.iloc[0].input_ids[i])}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f9fbd44-6baa-4c90-b6a6-52bff26164fe",
   "metadata": {},
   "source": [
    "df['labels_regressor'][3687]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "060c4b2f-49f2-4fdb-8b35-b4058fba5488",
   "metadata": {},
   "source": [
    "df['labels_classifier'][3687]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
